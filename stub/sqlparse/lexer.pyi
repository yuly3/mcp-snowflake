"""
This type stub file was generated by pyright.
"""

"""SQL Lexer"""
class Lexer:
    """The Lexer supports configurable syntax.
    To add support for additional keywords, use the `add_keywords` method."""
    _default_instance = ...
    _lock = ...
    @classmethod
    def get_default_instance(cls): # -> Self:
        """Returns the lexer instance used internally
        by the sqlparse core functions."""
        ...
    
    def default_initialization(self): # -> None:
        """Initialize the lexer with default dictionaries.
        Useful if you need to revert custom syntax settings."""
        ...
    
    def clear(self): # -> None:
        """Clear all syntax configurations.
        Useful if you want to load a reduced set of syntax configurations.
        After this call, regexps and keyword dictionaries need to be loaded
        to make the lexer functional again."""
        ...
    
    def set_SQL_REGEX(self, SQL_REGEX): # -> None:
        """Set the list of regex that will parse the SQL."""
        ...
    
    def add_keywords(self, keywords): # -> None:
        """Add keyword dictionaries. Keywords are looked up in the same order
        that dictionaries were added."""
        ...
    
    def is_keyword(self, value): # -> tuple[Any, Any] | tuple[Any | _TokenType, Any]:
        """Checks for a keyword.

        If the given value is in one of the KEYWORDS_* dictionary
        it's considered a keyword. Otherwise, tokens.Name is returned.
        """
        ...
    
    def get_tokens(self, text, encoding=...): # -> Generator[tuple[_TokenType, str] | tuple[Any, Any] | tuple[Any | _TokenType, Any] | tuple[Any | _TokenType, str], Any, None]:
        """
        Return an iterable of (tokentype, value) pairs generated from
        `text`. If `unfiltered` is set to `True`, the filtering mechanism
        is bypassed even if filters are defined.

        Also preprocess the text, i.e. expand tabs and strip it if
        wanted and applies registered filters.

        Split ``text`` into (tokentype, text) pairs.

        ``stack`` is the initial stack (default: ``['root']``)
        """
        ...
    


def tokenize(sql, encoding=...): # -> Generator[tuple[_TokenType, str] | tuple[Any, Any] | tuple[Any | _TokenType, Any] | tuple[Any | _TokenType, str], Any, None]:
    """Tokenize sql.

    Tokenize *sql* using the :class:`Lexer` and return a 2-tuple stream
    of ``(token type, value)`` items.
    """
    ...

